---
title: "MiniBPEï¼šæ¢ç©¶Githubä¸Šæœ€ç®€å•çš„BPEå®ç°ä»£ç "
date: 2024-03-09T22:12:43.000+0800
tags:
  - LLM
  - NLP
  - BPE
categories: LLM
excerpt: æ¢ç©¶Githubä¸Šçš„çƒ­é—¨é¡¹ç›®ï¼ŒAndrej Karpathyå¤§ç¥å®ç°çš„æœ€ç®€BPEç®—æ³•ä»£ç ã€‚
index_img: http://images.zerolovesea.top/blog/llm.png
lang: zh
duration: 39min
---

ä¹‹å‰åœ¨[åŸºäºå­è¯çš„åˆ†è¯æ–¹æ³•ï¼šBPEç®—æ³•](https://zerolovesea.github.io/2024/01/01/%E5%9F%BA%E4%BA%8E%E5%AD%97%E8%AF%8D%E7%9A%84%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95%EF%BC%9ABPE%E7%AE%97%E6%B3%95/)ä¸€æ–‡ä¸­ç®€å•å®ç°äº†BPEçš„ç®—æ³•ï¼Œä¸Šä¸ªæœˆå‰OpenAIæ•°æ®ç§‘å­¦å®¶Andrej Karpathyå¤§ç¥åœ¨Githubä¸Šå®ç°äº†ç›®å‰æœ€ç²¾ç®€çš„BPEç®—æ³•çš„ä»£ç ï¼Œè¿™ä¸€é¡¹ç›®ç¬é—´å†²åˆ°äº†Githubæ—¥æ¦œå¹¶è¿ç»­éœ¸æ¦œäº†ä¸€å‘¨ã€‚ç›®å‰è¯¥é¡¹ç›®å·²ç»æ‹¥æœ‰7.6ké¢—æ˜Ÿã€‚ä»Šå¤©å°±æ¥æ¢ç©¶ä¸€ä¸‹ä»–å†™çš„ä»£ç ç»†èŠ‚ï¼Œå­¦ä¹ ä¸€ä¸‹å¤§ç¥çš„ä»£ç è§„èŒƒã€‚

# é¡¹ç›®ç»“æ„

é¡¹ç›®ç”±å››ä¸ªæ–‡ä»¶ç»„æˆï¼šä¸€ä¸ª`base.py`å®ç°`Tokenizer`çš„æŠ½è±¡åŒ–åŸºç±»ã€‚ä¸€ä¸ª`Tokenizer`é€šå¸¸åŒ…å«ä¸‰ä¸ªæ–¹æ³•ï¼šè®­ç»ƒï¼Œç¼–ç å’Œè§£ç ã€‚

`basic.py`ç»§æ‰¿äº†`base.py`ï¼Œé‡Œé¢çš„`BasicTokenizer`ç±»æ˜¯BPEç®—æ³•çš„æ ¸å¿ƒå®ç°æ¨¡å—ã€‚

ç¬¬ä¸‰ä¸ªæ–‡ä»¶æ˜¯`regex.py`ï¼Œé‡Œé¢çš„`RegexTokenizer`ç±»çš„ä½œç”¨æ˜¯ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›´å¥½çš„æ‹†åˆ†æ–‡æœ¬ã€‚è¿™ä¸€éƒ¨åˆ†ä¸€èˆ¬å‡ºç°åœ¨é¢„å¤„ç†é˜¶æ®µï¼Œè®©æ–‡æœ¬æŒ‰ç…§ä¸åŒç±»åˆ«ï¼ˆå­—æ¯ï¼Œæ•°å­—ï¼Œæ ‡ç‚¹ï¼‰è¿›è¡Œæ‹†åˆ†ã€‚

æœ€åä¸€ä¸ªæ–‡ä»¶æ˜¯`gpt4.py`ï¼Œå®ƒå®ç°äº†`GPT4Tokenizer`ç±»ï¼Œå¤ç°äº†`tiktoken`åº“ä¸­çš„`GPT-4`çš„`Tokenizer`ã€‚

# ä½¿ç”¨æ–¹æ³•

## è°ƒç”¨æ–¹æ³•

å…ˆä¸Šä½¿ç”¨æ–¹æ³•ï¼š

```python
from minbpe import BasicTokenizer
tokenizer = BasicTokenizer()
text = "aaabdaaabac"
tokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges
print(tokenizer.encode(text))
# [258, 100, 258, 97, 99]
print(tokenizer.decode([258, 100, 258, 97, 99]))
# aaabdaaabac
tokenizer.save("toy")
```

å®ƒçš„ä½œç”¨æ˜¯å°†aaabdaaabacè¿›è¡Œä¸‰æ¬¡çš„åˆå¹¶ã€‚å®ƒä¼šè¾“å‡ºä¸€ä¸ªæ–°çš„å­—ç¬¦ä¸²XdXacï¼Œå…¶ä¸­X=ZYã€Y=ab å’Œ Z=aaã€‚minbpeå°†å•ç‹¬çš„256ä¸ªå­—èŠ‚åˆ†é…ä¸ºTokenï¼Œå› æ­¤åˆå¹¶åçš„æ–°å­—èŠ‚å°†ä»257å¼€å§‹ã€‚

ä¸Šè¿°çš„ä¾‹å­ä¸­ï¼Œa=97ã€b=98ã€c=99ã€d=100ï¼ˆå®ƒä»¬çš„ ASCII å€¼ï¼‰ã€‚ç„¶åï¼Œå½“ ï¼ˆaï¼Œaï¼‰ åˆå¹¶åˆ° Z æ—¶ï¼ŒZ å°†å˜ä¸º 256ã€‚åŒæ ·ï¼ŒY å°†å˜ä¸º 257 å’Œ X 258ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä» 256 ä¸ªå­—èŠ‚å¼€å§‹ï¼Œè¿›è¡Œ 3 æ¬¡åˆå¹¶ä»¥è·å¾—ä¸Šè¿°ç»“æœï¼Œé¢„æœŸè¾“å‡ºä¸º [258ï¼Œ 100ï¼Œ 258ï¼Œ 97ï¼Œ 99]ã€‚

å†ç”¨é¡¹ç›®å®ç°çš„`GPT4Tokenizer`å’Œ`tiktoken`å®ç°çš„`GPT-4 tokenizer`è¿›è¡Œå¯¹æ¯”ï¼š

```python
text = "hello123!!!? (ì•ˆë…•í•˜ì„¸ìš”!) ğŸ˜‰"

# tiktoken
import tiktoken
enc = tiktoken.get_encoding("cl100k_base")
print(enc.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

# ours
from minbpe import GPT4Tokenizer
tokenizer = GPT4Tokenizer()
print(tokenizer.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]
```

## è®­ç»ƒæ–¹æ³•

è¿™ä¸ªé¡¹ç›®é‡Œå®ç°äº†ä¸¤ç§æ–¹æ³•ï¼Œä¸€ç§æ–¹æ³•æ˜¯ä¸ä½¿ç”¨æ­£åˆ™æ¥å¤„ç†åŸæ–‡æœ¬ï¼Œè¿™æ—¶å€™ç›´æ¥ä½¿ç”¨`BasicTokenizer`è¿›è¡Œè®­ç»ƒã€‚

```python
from minbpe import BasicTokenizer
tokenizer = BasicTokenizer()
tokenizer.train(very_long_training_string, vocab_size=4096)
tokenizer.encode("hello world") # string -> tokens
tokenizer.decode([1000, 2000, 3000]) # tokens -> string
tokenizer.save("mymodel") # writes mymodel.model and mymodel.vocab
tokenizer.load("mymodel.model") # loads the model back, the vocab is just for vis
```

å¦‚æœè¦ä½¿ç”¨æ­£åˆ™æ–¹æ³•æ¥æŒ‰ç±»åˆ«æ‹†åˆ†æ–‡æœ¬ï¼Œå°±ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š

```python
from minbpe import RegexTokenizer
tokenizer = RegexTokenizer()
tokenizer.train(very_long_training_string, vocab_size=32768)
tokenizer.encode("hello world") # string -> tokens
tokenizer.decode([1000, 2000, 3000]) # tokens -> string
tokenizer.save("tok32k") # writes tok32k.model and tok32k.vocab
tokenizer.load("tok32k.model") # loads the model back from disk
```

å¦‚æœè¦æ·»åŠ `special tokens`ï¼Œä»£ç é‡Œä¹Ÿå®ç°äº†ä¸€ä¸ªæ–¹æ³•æ¥æ³¨å†Œï¼š

```python
from minbpe import RegexTokenizer
tokenizer = RegexTokenizer()
tokenizer.train(very_long_training_string, vocab_size=32768)
tokenizer.register_special_tokens({"<|endoftext|>": 32768})
tokenizer.encode("<|endoftext|>hello world", allowed_special="all")
```

# ä»£ç åˆ†æ

## base.py

è¯¥æ–‡ä»¶å®ç°äº†Toknizerçš„åŸºç±»ä»¥åŠå…¶ä»–éœ€è¦çš„å·¥å…·å‡½æ•°ã€‚

```python
import unicodedata

def get_stats(ids, counts=None):
    """
    è¾“å…¥ä¸€ä¸ªæ•´æ•°åˆ—è¡¨ï¼Œè¿”å›ä¸€ä¸ªè¿ç»­å¯¹çš„è®¡æ•°å­—å…¸
    ä¾‹å¦‚ï¼š[1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}
    å¯ä»¥é€‰æ‹©æ›´æ–°ä¸€ä¸ªå·²å­˜åœ¨çš„è®¡æ•°å­—å…¸
    """
    # å¦‚æœæ²¡æœ‰ä¼ å…¥countsï¼Œåˆ™åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—å…¸
    counts = {} if counts is None else counts  
    
    # éå†åˆ—è¡¨çš„å‰åè¿ç»­å¯¹ï¼šä¸æ–­è¿­ä»£å‰ä¸€ä¸ªå’Œåä¸€ä¸ªå­—ç¬¦å¯¹
    for pair in zip(ids, ids[1:]): 
        counts[pair] = counts.get(pair, 0) + 1 # ä¸æ–­å¢åŠ ç»Ÿè®¡æ•°
    return counts
```

`get_stats`å®é™…ä¸Šæ˜¯ä¸€ä¸ªæ¯”è¾ƒæ ¸å¿ƒçš„å‡½æ•°ï¼Œä¸‹é¢æ˜¯`merge`å‡½æ•°ï¼Œå°†æ‰€æœ‰çš„pairéƒ½ç”¨æŒ‡å®šçš„idxæ¥ä»£æ›¿ã€‚

```python
def merge(ids, pair, idx):
    """
    åœ¨æ•´æ•°åˆ—è¡¨ä¸­ï¼Œç”¨æ–°çš„æ•´æ•°idxæ›¿æ¢æ‰€æœ‰è¿ç»­å‡ºç°çš„pair
    ä¾‹å¦‚ï¼šids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]
    """
    newids = []
    i = 0
    while i < len(ids):
        # å¦‚æœä¸æ˜¯åœ¨æœ€åä¸€ä¸ªä½ç½®ï¼Œå¹¶ä¸”pairåŒ¹é…ï¼Œå°±æ›¿æ¢
        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:
            newids.append(idx) # æ–°åˆ—è¡¨æ·»åŠ 
            i += 2 # æŒ‡é’ˆå¢åŠ 2ä½
        else:
        	# å¦‚æœæ‰¾ä¸åˆ°åŒ¹é…çš„ï¼Œå°±æŠŠåŸå­—ç¬¦åŠ å…¥æ–°åˆ—è¡¨
            newids.append(ids[i]) 
            i += 1
    return newids
```

æ¥ä¸‹æ¥æ˜¯ä¸€ä¸ªè¾…åŠ©å‡½æ•°ã€‚åœ¨Unicodeä¸­ï¼ŒåŒ…å«ä¸€ç³»åˆ—æ§åˆ¶å­—ç¬¦ã€‚è¿™æ˜¯ä¸€ç»„ç‰¹æ®Šçš„å­—ç¬¦ï¼Œç”¨äºæ§åˆ¶æ–‡æœ¬çš„æ˜¾ç¤ºå’Œå¤„ç†ï¼Œè¿™äº›å­—ç¬¦é€šå¸¸ä¸å¯è§ã€‚æ§åˆ¶å­—ç¬¦çš„UnicodeèŒƒå›´æ˜¯U+0000è‡³U+001Få’ŒU+007Fè‡³U+009Fã€‚ç”±äºæˆ‘ä»¬å°†å­—ç¬¦ç¼–ç ä½Unicodeï¼Œç›®æ ‡è¯è¡¨ä¸­ï¼Œä¸éœ€è¦è¿™äº›æ§åˆ¶å­—ç¬¦ï¼Œæ‰€ä»¥éœ€è¦åˆ é™¤å®ƒä»¬ã€‚

```python
def replace_control_characters(s: str) -> str:
	"""
	å°†è¾“å…¥æ–‡æœ¬ä¸­ï¼Œæ‰€æœ‰çš„æ§åˆ¶å­—ç¬¦åˆ é™¤ï¼Œå¹¶è¿”å›å¤„ç†è¿‡åçš„å­—ç¬¦ä¸²ã€‚
	"""
    chars = []
    for ch in s:
        if unicodedata.category(ch)[0] != "C":
            chars.append(ch) # åªè¦ä¸æ˜¯æ§åˆ¶å­—ç¬¦å°±æ·»åŠ 
        else:
            chars.append(f"\\u{ord(ch):04x}") # å…¶è½¬æ¢ä¸ºUnicodeè½¬ä¹‰åºåˆ—
    return "".join(chars)

def render_token(t: bytes) -> str:
    """
    å°†bytesè½¬ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶æ¸…ç†æ§åˆ¶å­—ç¬¦
    """
    s = t.decode('utf-8', errors='replace')
    s = replace_control_characters(s)
    return s
```

å·¥å…·å‡½æ•°å†™å®Œäº†ï¼Œä¸‹é¢æ˜¯`Tokenizer`çš„æŠ½è±¡ç±»ï¼ŒæŠ½è±¡ç±»åŒ…å«äº†è®­ç»ƒï¼Œç¼–ç ï¼Œè§£ç ï¼Œæ„å»ºè¯è¡¨ï¼Œä¿å­˜å’ŒåŠ è½½æ–¹æ³•ã€‚

```python
class Tokenizer:
    """Base class for Tokenizers"""

    def __init__(self):
        # default: vocab size of 256 (all bytes), no merges, no patterns
        self.merges = {} # (int, int) -> int
        self.pattern = "" # str
        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}
        self.vocab = self._build_vocab() # int -> bytes

    def train(self, text, vocab_size, verbose=False):
        raise NotImplementedError

    def encode(self, text):
        raise NotImplementedError

    def decode(self, ids):
 
        raise NotImplementedError

    def _build_vocab(self):
        # æ„å»ºè¯è¡¨ï¼ŒåŸºç¡€è¯è¡¨æ˜¯256ä¸ªå­—èŠ‚
        vocab = {idx: bytes([idx]) for idx in range(256)}
        
        # (p0,p1) æ˜¯pairs
        for (p0, p1), idx in self.merges.items():
            vocab[idx] = vocab[p0] + vocab[p1]
        for special, idx in self.special_tokens.items():
            vocab[idx] = special.encode("utf-8")
        return vocab

    def save(self, file_prefix):
        """
        ä¿å­˜ä¸¤ä¸ªæ–‡ä»¶ï¼šfile_prefix.vocab å’Œ file_prefix.model
        - modelæ–‡ä»¶ç”¨äºload()
        - vocabæ–‡ä»¶åªæ˜¯ä¸€ä¸ªæ‰“å°ç‰ˆæœ¬ï¼Œä»…ä¾›äººç±»æ£€æŸ¥
        """
        # å†™å…¥æ–‡ä»¶
        model_file = file_prefix + ".model"
        with open(model_file, 'w') as f:
            # å†™å…¥ç‰ˆæœ¬ï¼Œæ¨¡å¼å’Œåˆå¹¶
            f.write("minbpe v1\n")
            f.write(f"{self.pattern}\n")
            # å†™å…¥ç‰¹æ®Šå­—ç¬¦
            f.write(f"{len(self.special_tokens)}\n")
            for special, idx in self.special_tokens.items():
                f.write(f"{special} {idx}\n")
            # åˆå¹¶å­—å…¸
            for idx1, idx2 in self.merges:
                f.write(f"{idx1} {idx2}\n")
                
        # å†™å…¥è¯è¡¨ï¼Œè¿™ä¸ªåªæ˜¯ç”¨æ¥çœ‹çš„
        vocab_file = file_prefix + ".vocab"
        inverted_merges = {idx: pair for pair, idx in self.merges.items()}
        with open(vocab_file, "w", encoding="utf-8") as f:
            for idx, token in self.vocab.items():
                s = render_token(token)
              
                if idx in inverted_merges:
                    idx0, idx1 = inverted_merges[idx]
                    s0 = render_token(self.vocab[idx0])
                    s1 = render_token(self.vocab[idx1])
                    f.write(f"[{s0}][{s1}] -> [{s}] {idx}\n")
                else:
                    f.write(f"[{s}] {idx}\n")

    def load(self, model_file):
        """è¯»å–æ¨¡å‹æ–‡ä»¶"""
        assert model_file.endswith(".model")
        # è¯»å–æ¨¡å‹æ–‡ä»¶
        merges = {}
        special_tokens = {}
        idx = 256
        with open(model_file, 'r', encoding="utf-8") as f:
            version = f.readline().strip()
            assert version == "minbpe v1"
            self.pattern = f.readline().strip()

            num_special = int(f.readline().strip())
            for _ in range(num_special):
                special, special_idx = f.readline().strip().split()
                special_tokens[special] = int(special_idx)

            for line in f:
                idx1, idx2 = map(int, line.split())
                merges[(idx1, idx2)] = idx
                idx += 1
        self.merges = merges
        self.special_tokens = special_tokens
        self.vocab = self._build_vocab()
```

## basic.py

è¯¥æ–‡ä»¶å®ç°äº†åŸºæœ¬çš„`Tokenizer`ç±»ã€‚é¦–å…ˆæ–­è¨€è¯è¡¨çš„å¤§å°å¤§äº256ï¼Œå¹¶è®¡ç®—è¦è¿›è¡Œå‡ æ¬¡mergeï¼Œä¹Ÿå°±æ˜¯å»æ‰256ä¸ªåŸºæœ¬å­—èŠ‚åï¼Œè¯è¡¨è¿˜å‰©ä¸‹å‡ ä¸ªã€‚

```python
from .base import Tokenizer, get_stats, merge

class BasicTokenizer(Tokenizer):

    def __init__(self):
        super().__init__()

    def train(self, text, vocab_size, verbose=False):
        assert vocab_size >= 256
        # è®¡ç®—mergeæ¬¡æ•°
        num_merges = vocab_size - 256

        # è¾“å…¥æ–‡æœ¬é¢„å¤„ç†
        text_bytes = text.encode("utf-8") # å°†æ–‡æœ¬è§£ç ä¸ºUtf-8æ ¼å¼
        ids = list(text_bytes) # æ¯ä¸ªå…ƒç´ éƒ½æ˜¯0-255ä¹‹é—´çš„æ•´æ•°çš„åˆ—è¡¨

        # è¿­ä»£åœ°åˆå¹¶æœ€å¸¸è§çš„pairï¼Œåˆ›å»ºæ–°çš„token
        merges = {} # (int, int) -> int
        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes
        for i in range(num_merges):
            # ç»Ÿè®¡æ¯ä¸ªpairå‡ºç°çš„æ¬¡æ•°ï¼Œè¿”å›å­—å…¸ï¼Œkeyæ˜¯pairï¼Œvalueæ˜¯å‡ºç°çš„æ¬¡æ•°
            stats = get_stats(ids)
            # æ‰¾åˆ°å‡ºç°æ¬¡æ•°æœ€å¤šçš„pair
            pair = max(stats, key=stats.get)
            # ä¸ºæ–°çš„tokenåˆ†é…ä¸€ä¸ªæ–°çš„id
            idx = 256 + i
            # ç”¨idxæ›¿æ¢idsä¸­æ‰€æœ‰çš„pair
            ids = merge(ids, pair, idx)
            # ä¿å­˜åˆå¹¶
            merges[pair] = idx
            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]
            # æ‰“å°
            if verbose:
                print(f"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences")

        # ä¿å­˜ç±»å˜é‡
        self.merges = merges # used in encode()
        self.vocab = vocab   # used in decode()

    def decode(self, ids):
        # è§£ç ï¼Œè¾“å…¥intç»„æˆçš„åˆ—è¡¨ï¼Œè¿”å›å­—ç¬¦ä¸²
        text_bytes = b"".join(self.vocab[idx] for idx in ids)
        text = text_bytes.decode("utf-8", errors="replace")
        return text

    def encode(self, text):
        # ç¼–ç ï¼Œè¾“å…¥å­—ç¬¦ä¸²ï¼Œè¿”å›intåˆ—è¡¨
        text_bytes = text.encode("utf-8") # raw bytes
        ids = list(text_bytes) # 0-255 intå€¼çš„åˆ—è¡¨
        while len(ids) >= 2:
            # æ‰¾åˆ°pairä¸­merge indexæœ€å°çš„pair
            stats = get_stats(ids)
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))
            # å¦‚æœæ²¡æœ‰æ›´å¤šçš„mergeå¯ç”¨ï¼Œé‚£ä¹ˆkeyå°†ç»™æ¯ä¸ªpairä¸€ä¸ªinfï¼Œminå°†æ˜¯åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªpair
            if pair not in self.merges:
                break # æ²¡æœ‰å¯ä»¥ç»§ç»­mergeçš„æƒ…å†µä¸‹ä¸­æ–­
            # å¦åˆ™ç»§ç»­mergeå½“å‰æœ€ä½³çš„pairï¼ˆæœ€å°‘mergeæ¬¡æ•°çš„indexï¼‰ 
            idx = self.merges[pair]
            ids = merge(ids, pair, idx)
        return ids
```

## regex.py

è¯¥æ–‡ä»¶å®ç°äº†`RegexTokenizer`ï¼Œæ˜¯ä¸€ä¸ªæ­£åˆ™å¤„ç†çš„ç±»ï¼Œç”¨äºé¢„å¤„ç†æ–‡æœ¬ï¼Œå¹¶å¤„ç†`special tokens`ã€‚

```python
import regex as re
from .base import Tokenizer, get_stats, merge

# GPTæ–‡æœ¬çš„åˆ†è¯å¤„ç†æ¨¡å¼
# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py
GPT2_SPLIT_PATTERN = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
GPT4_SPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""

class RegexTokenizer(Tokenizer):

    def __init__(self, pattern=None):
        """
        - pattern: å¯é€‰çš„å­—ç¬¦ä¸²ï¼Œç”¨äºè¦†ç›–é»˜è®¤çš„ï¼ˆGPT-4åˆ†å‰²æ¨¡å¼ï¼‰
        - special_tokens: ç‰¹æ®Štokençš„str -> intå­—å…¸
          ä¾‹å¦‚ï¼š{'<|endoftext|>': 100257}
        """
        super().__init__()
        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern
        self.compiled_pattern = re.compile(self.pattern)
        self.special_tokens = {} # str -> int
        self.inverse_special_tokens = {} # int -> str

    def train(self, text, vocab_size, verbose=False):
        assert vocab_size >= 256
        num_merges = vocab_size - 256

        # åˆ†å‰²æ–‡æœ¬ä¸ºæ–‡æœ¬å—
        text_chunks = re.findall(self.compiled_pattern, text)

        # è¾“å…¥æ–‡æœ¬é¢„å¤„ç†
        ids = [list(ch.encode("utf-8")) for ch in text_chunks]

        # è¿­ä»£å°†æœ€å¸¸è§çš„ç»„åˆåˆå¹¶ä¸ºæ–°çš„æ ‡è®°
        merges = {} # (int, int) -> int
        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes
        for i in range(num_merges):
           # è®¡ç®—æ¯ä¸ªè¿ç»­ç»„åˆå‡ºç°çš„æ¬¡æ•°
            stats = {}
            for chunk_ids in ids:
                # ä¼ å…¥statså°†åœ¨åŸåœ°æ›´æ–°å®ƒï¼Œç´¯åŠ è®¡æ•°
                get_stats(chunk_ids, stats)
            # æ‰¾åˆ°è®¡æ•°æœ€é«˜çš„ç»„åˆ
            pair = max(stats, key=stats.get)
            # é“¸é€ ä¸€ä¸ªæ–°çš„æ ‡è®°ï¼šåˆ†é…ä¸‹ä¸€ä¸ªå¯ç”¨çš„id
            idx = 256 + i
            # ç”¨idxæ›¿æ¢idsä¸­æ‰€æœ‰pairçš„å‡ºç°
            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]
            # ä¿å­˜merge
            merges[pair] = idx
            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]
            # æ‰“å°
            if verbose:
                print(f"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences")

        # ä¿å­˜
        self.merges = merges # used in encode()
        self.vocab = vocab   # used in decode()

    def register_special_tokens(self, special_tokens):
        # special_tokens: ä¸€ä¸ªç‰¹æ®Šçš„å­—å…¸ str -> int
        # ä¾‹å¦‚: {"<|endoftext|>": 100257}
        self.special_tokens = special_tokens
        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}

    def decode(self, ids):
        part_bytes = []
        for idx in ids:
            if idx in self.vocab:
                part_bytes.append(self.vocab[idx])
            elif idx in self.inverse_special_tokens:
                part_bytes.append(self.inverse_special_tokens[idx].encode("utf-8"))
            else:
                raise ValueError(f"invalid token id: {idx}")
        text_bytes = b"".join(part_bytes)
        text = text_bytes.decode("utf-8", errors="replace")
        return text

    def _encode_chunk(self, text_bytes):
        # è¿”å› token ids
        # å°†æ‰€æœ‰å­—èŠ‚è½¬æ¢ä¸º0..255èŒƒå›´å†…çš„æ•´æ•°
        ids = list(text_bytes)
        while len(ids) >= 2:
            # æ‰¾åˆ°pairä¸­merge indexæœ€å°çš„pair
            stats = get_stats(ids)
            pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))
            if pair not in self.merges:
                break 

            idx = self.merges[pair]
            ids = merge(ids, pair, idx)
        return ids

    def encode_ordinary(self, text):
        """ç¼–ç å¹¶å¿½ç•¥ä»»ä½•special tokenã€‚"""
        # æŒ‰ç…§æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ä¸­å®šä¹‰çš„ç±»åˆ«å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ–‡æœ¬å—
        text_chunks = re.findall(self.compiled_pattern, text)
        # æ‰€æœ‰å­—ç¬¦å—è¢«å•ç‹¬ç¼–ç ï¼Œå¹¶åœ¨æœ€ååˆå¹¶
        ids = []
        for chunk in text_chunks:
            chunk_bytes = chunk.encode("utf-8") # raw bytes
            chunk_ids = self._encode_chunk(chunk_bytes)
            ids.extend(chunk_ids)
        return ids

    def encode(self, text, allowed_special="none_raise"):
        """
        ä¸encode_ordinaryä¸åŒï¼Œæ­¤å‡½æ•°å¤„ç†ç‰¹æ®Štokenã€‚
        allowed_special: å¯ä»¥æ˜¯"all"|"none"|"none_raise"æˆ–ç‰¹æ®Štokençš„è‡ªå®šä¹‰é›†åˆ
        å¦‚æœnone_raiseï¼Œåˆ™åœ¨æ–‡æœ¬ä¸­é‡åˆ°ä»»ä½•ç‰¹æ®Štokenæ—¶ä¼šå¼•å‘é”™è¯¯
        """
        # decode the user desire w.r.t. handling of special tokens
        special = None
        if allowed_special == "all":
            special = self.special_tokens
        elif allowed_special == "none":
            special = {}
        elif allowed_special == "none_raise":
            special = {}
            assert all(token not in text for token in self.special_tokens)
        elif isinstance(allowed_special, set):
            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}
        else:
            raise ValueError(f"allowed_special={allowed_special} not understood")
        if not special:
            # å¦‚æœæ²¡æœ‰special token,å°±ä½¿ç”¨ordinary encoding
            return self.encode_ordinary(text)
        # å¦åˆ™ï¼Œæˆ‘ä»¬å¿…é¡»å°å¿ƒå¤„ç†æ–‡æœ¬ä¸­å¯èƒ½çš„ç‰¹æ®Štoken
        # æˆ‘ä»¬é€šè¿‡åœ¨æ–‡æœ¬ä¸­å‡ºç°ä»»ä½•ç‰¹æ®Štokençš„ç¡®åˆ‡åŒ¹é…æ¥å¤„ç†ç‰¹æ®Štoken
        # æˆ‘ä»¬å¯ä»¥ä½¿ç”¨re.splitæ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¯·æ³¨æ„ï¼Œå°†æ¨¡å¼æ‹¬åœ¨()ä¸­
        # ä½¿å…¶æˆä¸ºæ•è·ç»„ï¼Œå› æ­¤ç‰¹æ®Štokenå°†è¢«åŒ…æ‹¬åœ¨å†…
        special_pattern = "(" + "|".join(re.escape(k) for k in special) + ")"
        special_chunks = re.split(special_pattern, text)
        # ç°åœ¨æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦éƒ½ä¸æ–‡æœ¬çš„å…¶ä½™éƒ¨åˆ†åˆ†å¼€
        # æ‰€æœ‰æ–‡æœ¬å—éƒ½æ˜¯åˆ†å¼€ç¼–ç çš„ï¼Œç„¶åç»“æœæ˜¯è¿æ¥çš„
        ids = []
        for part in special_chunks:
            if part in special:
                # è¿™æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æ ‡è®°ï¼Œå°†å…¶å•ç‹¬ç¼–ç ä¸ºç‰¹æ®Šæƒ…å†µ
                ids.append(special[part])
            else:
                # è¿™æ˜¯ä¸€ä¸ªæ™®é€šçš„åºåˆ—ï¼Œæ­£å¸¸ç¼–ç 
                ids.extend(self.encode_ordinary(part))
        return ids
```

## gpt4.py

æœ€åä¸€ä¸ªæ–‡ä»¶æ˜¯`gpt4.py`ï¼Œå®ç°äº†åŸºäº`RegexTokenizer`çš„`GPT4Tokenizer`ã€‚

```python
import tiktoken
from .regex import RegexTokenizer


def bpe(mergeable_ranks, token, max_rank):
    # è¾…åŠ©å‡½æ•°ï¼Œç”¨äºåœ¨get_gpt4_merges()ä¸­é‡æ„åˆå¹¶æ ‘
    parts = [bytes([b]) for b in token]
    while True:
        min_idx = None
        min_rank = None
        for i, pair in enumerate(zip(parts[:-1], parts[1:])):
            rank = mergeable_ranks.get(pair[0] + pair[1])
            if rank is not None and (min_rank is None or rank < min_rank):
                min_idx = i
                min_rank = rank
        if min_rank is None or (max_rank is not None and min_rank >= max_rank):
            break
        assert min_idx is not None
        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]
    return parts


def recover_merges(mergeable_ranks):
    # `merges`å·²ç»æ˜¯å®ƒä»¬åˆå¹¶çŠ¶æ€çš„å­—èŠ‚åºåˆ—ã€‚
    # å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»æ¢å¤åŸå§‹çš„é…å¯¹ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹æ‰€æœ‰tokenè¿›è¡Œä¸€æ¬¡å°å‹BPEè®­ç»ƒæ¥å®ç°è¿™ä¸€ç‚¹ï¼ŒæŒ‰é¡ºåºè¿›è¡Œã€‚
  
    merges = {}
    for token, rank in mergeable_ranks.items():
        if len(token) == 1:
            continue # skip raw bytes
        pair = tuple(bpe(mergeable_ranks, token, max_rank=rank))
        assert len(pair) == 2
        # æ¢å¤å¯¹çš„æ•´æ•°ç­‰çº§
        ix0 = mergeable_ranks[pair[0]]
        ix1 = mergeable_ranks[pair[1]]
        merges[(ix0, ix1)] = rank

    return merges
```

ä¸‹é¢æ˜¯`GPT4Tokenizer`çš„å…·ä½“å®ç°ï¼š

```python
GPT4_SPLIT_PATTERN = r"""'(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+"""
GPT4_SPECIAL_TOKENS = {
    '<|endoftext|>': 100257,
    '<|fim_prefix|>': 100258,
    '<|fim_middle|>': 100259,
    '<|fim_suffix|>': 100260,
    '<|endofprompt|>': 100276
}

class GPT4Tokenizer(RegexTokenizer):
    """RegexTokenizerçš„è½»é‡çº§åŒ…è£…å™¨ï¼ŒåŒ¹é…GPT-4çš„åˆ†è¯å™¨ã€‚"""

    def __init__(self):
        super().__init__(pattern=GPT4_SPLIT_PATTERN)
        # è·å–å®˜æ–¹tokenizerå’Œmerges
        enc = tiktoken.get_encoding("cl100k_base")
        mergeable_ranks = enc._mergeable_ranks
        # the merges are those of gpt4, but we have to recover them
        self.merges = recover_merges(mergeable_ranks)
        # ä»mergesé‡å»ºvocab
        vocab = {idx: bytes([idx]) for idx in range(256)}
        for (p0, p1), idx in self.merges.items():
            vocab[idx] = vocab[p0] + vocab[p1]
        self.vocab = vocab
        
        # ç”±äºæŸç§åŸå› ï¼Œä¸å•ä¸ªå­—èŠ‚å¯¹åº”çš„æ ‡è®°ä»¥ä¸åŒçš„é¡ºåºæ’åˆ—ã€‚
        self.byte_shuffle = {i: mergeable_ranks[bytes([i])] for i in range(256)}
        self.inverse_byte_shuffle = {v: k for k, v in self.byte_shuffle.items()}
		# æ³¨å†Œspecial tokens
        self.register_special_tokens(GPT4_SPECIAL_TOKENS)

    def _encode_chunk(self, text_bytes):
    	# åœ¨æˆ‘ä»¬å¼€å§‹å¤„ç†å­—èŠ‚ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹å®ƒä»¬è¿›è¡Œæ’åˆ—
        text_bytes = bytes(self.byte_shuffle[b] for b in text_bytes)
        ids = super()._encode_chunk(text_bytes)
        return ids

    def decode(self, ids):
    	# æˆ‘ä»¬å¿…é¡»åœ¨è§£ç ä¹‹å‰å¯¹å­—èŠ‚è¿›è¡Œåæ’åˆ—
        text_bytes = b"".join(self.vocab[idx] for idx in ids)
        text_bytes = bytes(self.inverse_byte_shuffle[b] for b in text_bytes)
        text = text_bytes.decode("utf-8", errors="replace")
        return text

    def train(self, text, vocab_size, verbose=False):
        raise NotImplementedError

    def save(self, file_prefix):
        raise NotImplementedError("GPT4Tokenizer cannot be saved.")

    def load(self, model_file):
        raise NotImplementedError("GPT4Tokenizer cannot be loaded.")

    def save_vocab(self, vocab_file):
        # ä»…ç”¨äºå¯è§†åŒ–ç›®çš„ï¼Œè®©æˆ‘ä»¬ä»¥ä¸åŸºç±»å®Œå…¨ç›¸åŒçš„æ ¼å¼è¾“å‡ºGPT-4æ ‡è®°ã€‚
        # ç®€å•è¿è¡Œï¼š
        # python -c "from minbpe import GPT4Tokenizer; GPT4Tokenizer().save_vocab('gpt4.vocab')"
    
        from .base import render_token
        vocab = {idx: bytes([self.inverse_byte_shuffle[idx]]) for idx in range(256)}
        for (p0, p1), idx in self.merges.items():
            vocab[idx] = vocab[p0] + vocab[p1]

        inverted_merges = {idx: pair for pair, idx in self.merges.items()}
        with open(vocab_file, "w", encoding="utf-8") as f:
            for idx, token in vocab.items():
                s = render_token(token)
                if idx in inverted_merges:
                    idx0, idx1 = inverted_merges[idx]
                    s0 = render_token(vocab[idx0])
                    s1 = render_token(vocab[idx1])
                    f.write(f"[{s0}][{s1}] -> [{s}] {idx}\n")
                else:
                    f.write(f"[{s}] {idx}\n")
```


2024/3/10 äºè‹å·
