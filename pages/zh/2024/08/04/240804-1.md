---
title: "推荐系统：逻辑回归-工业特征融合的中流砥柱"
date: 2024-08-04T15:49:13.000+0800
tags:
  - 推荐系统
  - 逻辑回归
  - 机器学习
categories: 推荐系统
excerpt: 逻辑回归在推荐算法领域的使用，以及其意义。
index_img: https://images.zerolovesea.top/blog/rec.png
lang: zh
duration: 2min
---

逻辑回归可以算得上是机器学习入门前几课必接触的内容了，作为线性回归的映射，它具有可解释性非常强的优点，并且在推荐算法工业界非常流行。

在推荐算法中，逻辑回归将推荐任务转换为一个二分类任务，即判断用户是否会点击。这也和CTR任务不谋而合。

逻辑回归的公式人尽皆知：

$P(y=1 \mid \mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x} - b}}$

其中$w$是权重向量，$\mathbf{x}$ 是特征向量，$b$ 是偏置项。损失函数是交叉熵损失。

逻辑回归基于输入特征的线性组合，将线性组合的输出映射到[0,1]区间的一个实数，这里的映射就是一个sigmoid函数。由于它的值域落在0-1区间，符合概率的意义，因此很早就被用在广告算法的CTR模型中。

Sigmoid函数之所以流行，不光是它的值域在0-1区间，而且它的导数非常好计算，是$f(x)(1-f(x))$。

# 为什么用逻辑回归

相比更具有数学之美的SVM和各种树模型，逻辑回归在推荐系统发展史上的地位举足轻重。

用它的理由主要有三点：

- 数学支持：逻辑回归基于广义线性模型，假设因变量服从伯努利分布，这和CTR任务相似。
- 可解释性强：每个特征都有对应的权重，无论是特征交叉还是单独特征，都可以通过权重大小直观的看到对于最终结果的贡献度
- 训练开销小：能够在不同机器上并行计算

逻辑回归的讲解，公式推导及梯度下降，网上有很多，在此不赘述。在非常早期的推荐系统中，往往需要人工构建特征，这导致诸多不便，尤其是需要考虑到特征交叉的情况。为了解决这个问题，出现了GBDT+LR的架构，以实现特征自动组合。这个先码住，下次再写。

2024/8/4 于苏州
