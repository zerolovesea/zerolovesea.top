---
title: "重温统计学习方法：极大似然估计与贝叶斯估计"
date: 2025-01-19T10:07:22.000+0800
tags:
  - 统计机器学习
  - MLE
  - 贝叶斯估计
categories: 统计机器学习
excerpt: 温习一下两者的区别。
index_img: https://images.zerolovesea.top/blog/statistic.jpg
lang: zh
duration: 9min
---



最近在重温统计机器学习，发现很多概念都已经很模糊了，一些算法的推导公式和原理都不记得了，因此决定重新整理一下。这次先从最开始的两大统计学派：频率学派和贝叶斯学派开始。

------

## **极大似然估计 (MLE)**

极大似然的基本思想是通过已有的观测到的数据，找到使观测数据出现的可能性最大的模型参数。

**原理**

1. 假设数据来自某一概率分布 $P(x|\theta)$ ，其中 $\theta$ 是待估计的参数。
2. 给定一组观测数据$\{x_1, x_2, \ldots, x_n\}$，似然函数定义为： $L(\theta) = P(x_1, x_2, \ldots, x_n | \theta)$ 
3. 我们假设数据独立同分布（i.i.d.），这样就变成了多个样本的联合概率，可以直接写成多个概率的乘积： $L(\theta) = \prod_{i=1}^n P(x_i|\theta)$
3. 取对数简化计算： $\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log P(x_i|\theta)$
4. 找到使对数似然函数最大的参数 $\theta$，即： $\theta_{MLE} = \arg\max_{\theta} \ell(\theta)$


MLE体现了传统频率学派的观点：只基于数据，不依赖先验分布。当样本量足够大的时候，它的期望等于实际参数。而在样本量不够，或者参数量过大的时候，又会导致过拟合。

------

## **贝叶斯估计**

和MLE对应的是贝叶斯估计。贝叶斯估计基于贝叶斯定理，将先验知识和观测数据相结合，得到参数的后验分布，并以此进行参数估计。

**原理**

1. 贝叶斯定理公式为：

   $P(\theta | x) = \frac{P(x | \theta) P(\theta)}{P(x)}$

   - $P(\theta)$：先验分布，表示对参数 $\theta$ 的先验知识。
   - $P(x|\theta)$：似然函数，表示在参数 $\theta$ 下观测数据 $x$ 的概率。
   - $P(x)$：归一化常数，通常不直接关心。
   - $P(\theta|x)$：后验分布，结合了先验知识和观测数据后的参数分布。

2. 根据后验分布进行估计：

   - **最大后验估计（MAP）**：取后验分布的最大值作为参数估计值： $\theta_{MAP} = \arg\max_{\theta} P(\theta | x)$
   - **后验均值估计**：计算后验分布的期望值： ${\theta}_{Bayes} = \int \theta P(\theta|x) d\theta$

贝叶斯估计体现了贝叶斯学派的观点：结合先验分布与观测数据。由于存在了一些先验的信息，在样本量不够的情况下比MLE效果更好。

------

## **极大似然估计与贝叶斯估计的区别**

| 特点           | 极大似然估计 (MLE)                 | 贝叶斯估计                   |
| -------------- | ---------------------------------- | ---------------------------- |
| **理论基础**   | 频率学派，关注观测数据             | 贝叶斯学派，结合先验与数据   |
| **输入信息**   | 仅使用观测数据                     | 使用观测数据和先验分布       |
| **目标**       | 最大化似然函数                     | 最大化后验分布               |
| **适用场景**   | 样本量大，先验信息不足或无先验信息 | 样本量小，存在可靠的先验信息 |
| **计算复杂度** | 较低，解析解常常可用               | 较高，可能需要数值积分或近似 |
| **估计结果**   | 点估计（一个确定值）               | 分布估计（后验分布）或点估计 |

------

## **更具象的解释**

但从理论解释可能还有一些混乱，用一个机器学习调参的任务来对比一下他们的区别。

对于一个机器学习模型，极大似然估计任务模型参数是一个固定值，需要对数据最大化似然来得到这个模型参数。而贝叶斯估计认为模型参数是随机的，先拥有一个先验的分布，随后根据数据，计算参数的后验分布。

1. 极大似然估计里，我们直接最大化对数似然函数，就能找到这个参数。

2. 在贝叶斯估计里，我们先对参数$\theta$有一个先验知识，我们假定权重参数都是0，并且认为这些参数符合正态分布，那么正态分布就是参数$\theta$的先验分布：$P(\theta) = \prod_{j=1}^d N(\theta_j; 0, \tau^2)$

3. 这时根据贝叶斯公式，后验分布就是先验分布和似然函数的乘积，对这个乘积求最大值已得到参数：$\theta_{\text{MAP}} = \arg\max_{\theta} \left( \log P(y \mid x; \theta) + \log P(\theta) \right)$

对比最大后验估计和极大似然估计，两者在取对数的时候，只相差了一个先验项$\log P(\theta) $。

## 个人疑问

个人的疑问在于：当用多个batch训练模型的时候，每个batch中的模型，是否都拥有了前一个batch的先验知识，包括模型和数据。

对于这个问题，我直接复制GPT的答案：

在极大似然估计（MLE）中，多个 batch 的数据和模型的更新并不能被称为“先验知识”，这是因为**MLE 本身没有引入先验概率的概念**。让我们详细分析这个问题：

------

### **极大似然估计的性质**

1. **没有先验知识**： 在 MLE 中，参数 $\theta$ 被认为是一个固定但未知的值。MLE 的目标是找到一个使数据的似然 $)P(x|\theta)$最大的 $\theta$。这里不涉及任何关于 $\theta$ 的先验分布或假设。

2. **每个 batch 的数据贡献**： 在使用多个 batch 的数据时，MLE 的思想是逐步更新参数，使得所有数据的对数似然总和最大化：

   $\ell(\theta) = \sum_{i=1}^n \log P(x_i | \theta)$

   在批量训练中，每个 batch 的数据只是全量数据的一个子集，对总对数似然做局部贡献。**这些数据只影响参数的更新，并不影响对 $\theta$ 的先验假设**。

3. **批次模型的状态**： 在 MLE 中，每个 batch 的训练结果仅仅是当前模型参数的更新，反映的是如何让模型更好地拟合已有数据，而不是引入任何“先验知识”。

------

### **贝叶斯估计的视角**

相比之下，贝叶斯估计明确区分了**先验知识**和**数据贡献**：

1. **先验分布$P(\theta)$**：
   - 贝叶斯估计在更新参数时，会显式考虑 $\theta$ 的先验分布，先验分布反映了在看到数据之前我们对参数的主观认识。
   - 先验分布是模型的“初始信念”，不依赖观测数据。
2. **后验更新**：
   - 贝叶斯方法结合先验分布和批量数据 $x_{batch}$ 计算后验分布： $P(θ)P(\theta | x_{batch}) \propto P(x_{batch}|\theta) P(\theta)$
   - 每一批数据 $x_{batch}$ 更新后，新的后验分布 $P(\theta|x_{batch})$就成了下一批数据的先验分布。
   - 这种“动态更新”明确体现了贝叶斯的先验和后验的关系。

------

### **MLE 与批量训练的关系**

在 MLE 中：

- 批量训练时，每个 batch 的数据只是对总似然的一个部分贡献，并不会改变对参数的任何“先验假设”，因为 MLE 没有先验假设。
- 批次之间的模型状态只是参数更新的结果，并不能被视为先验分布的动态更新。

------

### **为什么 MLE 的批量训练不等于先验知识？**

可以用以下几点来解释：

1. **先验知识的定义**：
   - 贝叶斯统计中的先验知识指的是在没有观察数据之前，对参数 $\theta$ 的主观信念或背景信息。
   - MLE 没有先验知识的概念，它的参数估计完全基于观测数据。
2. **批次间的独立性**：
   - 在 MLE 中，每个 batch 的数据对模型的贡献是独立的，并没有继承“先验到后验再成为新先验”的动态过程。
3. **更新过程的本质**：
   - 批量训练的参数更新（如梯度下降）是逐步逼近全数据最大似然解的一种数值优化方法，目的是最大化全数据的似然，而不是动态调整对参数的信念。

------

### **类比说明**

- **MLE 的批量训练**： 像是用分段的方法计算总和，每一批次的数据只是整个总量的一部分贡献，目的是找到能最大化全量数据似然的参数值。
- **贝叶斯方法**： 像是分步更新信念的过程，每一批数据都会更新对参数的信念（后验分布），并在下一批数据中用新的信念作为起点。

------

### **总结**

在 MLE 中，批次间的模型状态或数据贡献只是逐步逼近全数据最优解的过程，不能被称为“先验知识”。只有在贝叶斯估计中，才有明确的先验分布和动态更新的概念。批量训练在 MLE 和贝叶斯估计中的作用是完全不同的：

- **MLE**：所有数据共同构造一个似然函数，逐步优化其最大值。
- **贝叶斯估计**：每一批数据更新后验分布，后验分布成为下一步的先验分布，明确体现了“先验知识”的动态演化。

## 经验风险和结构风险

入门概念里有经验风险和结构风险的概念，前者在MLE中就是似然函数，指代已有数据下需要最小化的经验损失。后者就是正则项。当模型是条件概率分布，损失函数是对数损失，模型复杂度是模型的先验概率时，最大后验概率分布就是结构风险最小化：这恰恰就是MAP的公式。

2025/1/24 于苏州
